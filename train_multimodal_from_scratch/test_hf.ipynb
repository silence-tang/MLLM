{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "import requests\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"/data/vdc/tangzichen/qwen2.5\",\n",
    "    # local_dir_use_symlinks=False,\n",
    "    # torch_dtype=\"auto\",\n",
    "    # device_map=\"cuda:1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/data/vdc/tangzichen/qwen2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
       "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
       "             13, 151645,    198, 151644,    872,    198,  35127,    752,    264,\n",
       "           2805,  16800,    311,   3460,   4128,   1614,     13, 151645,    198,\n",
       "         151644,  77091,    198]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "model_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 39814,      0,    362,   3460,   4128,   1614,    320,   4086,     44,\n",
       "              8,    374,    264,    943,    315,  20443,  11229,    429,    646,\n",
       "           6923,   3738,  12681,   1467,   3118,    389,   1946,    821,    323,\n",
       "           4862,     13,   4220,   4119,    525,   6188,    311,    387,   2952,\n",
       "            311,   3535,   6351,   5810,  15459,    323,   6923,  55787,    323,\n",
       "           2266,   1832,   9760,  14507,    382,   3862,    525,   2155,   4494,\n",
       "            315,    444,  10994,     82,     11,   2670,   5538,   6832,   5980,\n",
       "           4119,   1075,  86870,     11,  64074,  29728,  14155,     11,    323,\n",
       "           1766,   1388,  28411,  42721,  14155,     11,    438,   1632,    438,\n",
       "            803,   8606,  19827,   1741,    438,   5912,   5980,   5942,    476,\n",
       "          28464,   4119,     13,    576,   1429,  11245,    444,  10994,     82,\n",
       "           2924,    279,    479,   2828,   4013,    504,  16580,  43936,     11,\n",
       "            892,    702,   1012,   1483,    304,   5257,   8357,     11,    323,\n",
       "            279,    425,   3399,   4013,    504,  54364,  14817,     11,    892,\n",
       "            572,   7881,    369,   1467,   9471,   9079,    382,  34253,   4128,\n",
       "           4119,    614,    279,   4650,    311,  13791,    551,   1657,  19102,\n",
       "             11,   2670,  18478,     11,  17017,     11,   5777,   3516,     11,\n",
       "            323,   6731,     13,   2379,    646,   1492,  68611,   1045,   9079,\n",
       "            323,   7269,    279,   4271,    315,   1995,   3897,    553,  12677,\n",
       "             13,   4354,     11,    432,    594,   2989,    311,   5185,    429,\n",
       "            279,    990,    315,    444,  10994,     82,   1265,   2677,    387,\n",
       "           2814,  86288,    323,   8372,   2673,     11,  22573,    807,    525,\n",
       "           1483,    369,   6785,   9895,    323,    537,  14381,  11428,    476,\n",
       "           5786,  83116,    315,    279,   3738,   3942,     13, 151645])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开头一些token是输入, 需要去除\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! A large language model (LLM) is a type of artificial intelligence that can generate human-like text based on input data and training. These models are designed to be able to understand complex natural languages and generate coherent and contextually relevant responses.\\n\\nThere are different types of LLMs, including deep learning-based models like transformers, recurrent neural networks, and generative adversarial networks, as well as more traditional approaches such as rule-based systems or statistical models. The most famous LLMs include the GPT series from Anthropic, which has been used in various applications, and the BERT series from Alibaba Cloud, which was developed for text generation tasks.\\n\\nLarge language models have the potential to revolutionize many industries, including healthcare, finance, legal services, and education. They can help automate some tasks and improve the quality of information provided by humans. However, it's important to note that the use of LLMs should always be done responsibly and ethically, ensuring they are used for positive purposes and not causing harm or misrepresentation of the human community.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tangzichen.tzc/anaconda3/envs/humangaussian/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained('google/siglip-base-patch16-224', cache_dir=\"/data/vdc/tangzichen/siglip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://images.cocodataset.org/val2017/000000039769.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m      4\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma photo of 2 cats\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma photo of 2 dogs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mtexts, images\u001b[38;5;241m=\u001b[39mimage, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n",
    "\n",
    "inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humangaussian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
